<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Saahil Khanna | Research Projects</title>
  <style>
    :root {
      --bg-color: #121212;
      --text-color: #e0e0e0;
      --accent-color: #00bcd4;
      --card-bg: #1e1e1e;
      --border-color: #2c2c2c;
    }

    body {
      margin: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background-color: var(--bg-color);
      color: var(--text-color);
    }

    header {
      background: #000;
      text-align: center;
      padding: 2rem 1rem;
    }

    header h1 {
      margin: 0;
      font-size: 2rem;
      color: var(--accent-color);
    }

    header p {
      margin-top: 0.5rem;
    }

    a {
      color: var(--accent-color);
      text-decoration: none;
    }

    .container {
      max-width: 960px;
      margin: 0 auto;
      padding: 2rem 1rem;
    }

    .project {
      background-color: var(--card-bg);
      padding: 1.5rem;
      margin-bottom: 4rem;
      border-radius: 10px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);
    }

    .project h2 {
      color: var(--accent-color);
      margin-top: 0;
    }

    .project h3 {
      margin-top: 1.5rem;
      color: #90caf9;
    }

    .project img {
      width: 100%;
      margin: 1rem 0;
      border-radius: 8px;
      border: 1px solid var(--border-color);
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-size: 0.95rem;
      color: #ddd;
    }

    th, td {
      border: 1px solid var(--border-color);
      padding: 0.6rem;
      text-align: center;
    }

    th {
      background-color: #2c2c2c;
    }

    footer {
      text-align: center;
      padding: 1rem;
      background: #000;
      color: #777;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <header>
    <h1>Cybersecurity Projects</h1>
    <h1>Saahil Khanna</h1>
    <p>Email: <a href="mailto:saahilkhanna182000@gmail.com">saahilkhanna182000@gmail.com</a></p>
  </header>

  <div class="container">

    <div class="project">
      <h2>üìò Robust Monocular Depth Estimation Against Adversarial Camouflage</h2>

      <h3>Abstract</h3>
      <p>Monocular Depth Estimation (MDE) is increasingly used in autonomous vehicles due to its low cost and scalability. However, MDE models are vulnerable to adversarial attacks, which can degrade depth predictions and pose safety risks. In this project, we assess the vulnerability of top MDE models ‚Äî Monodepth2, DenseDepth, DepthAnythingV2, and ZoeDepth ‚Äî to optimized 2D adversarial patches based on the 3D2Fool algorithm. Our findings highlight significant variations in model robustness. To address this, we implement adversarial training, which improves resilience and reduces error, helping secure perception systems in AVs.</p>

      <h3>Methodology</h3>
      <p>The methodology includes four stages: creating a synthetic dataset using the CARLA simulator, optimizing adversarial patches using a custom loss function, evaluating several MDE models under attack, and applying adversarial training to improve robustness.</p>
      <ul>
        <li><strong>Dataset:</strong> 10,500 synthetic images of a 3D car model across varied scenes and camera angles, including weather variations.</li>
        <li><strong>Patch Optimization:</strong> A custom loss function was designed to distort depth predictions. Gradient descent was used to maximize this distortion.</li>
        <li><strong>Object Detection & Placement:</strong> YOLOv8 was used for robust vehicle detection and patch placement.</li>
        <li><strong>Adversarial Training:</strong> DenseDepth was fine-tuned using patched images to enhance resistance without degrading clean image performance.</li>
      </ul>
      <img src="images/Background Images (1) (1).png" alt="Methodology Diagram">

      <h3>Sample Visualizations</h3>
      <img src="images/a (1).png" alt="Unpatched and Patched Comparison">

      <h3>üß™ Experiments</h3>
      <h4>a) Testing Adversarial Patch Effectiveness Across MDE Models</h4>
      <p>We tested our optimized adversarial patch and another benchmark patch (SPOO) on seven state-of-the-art monocular depth estimation models: Monodepth2, DenseDepth, ZoeDepth, DPT, DepthAnythingV2, AdaBins, and MiDaS.<br>
      üìä Metrics like Mean Depth Estimation Error (Ed), Ratio of Affected Region (Ra), and SSIM were used to evaluate the impact.</p>
      <ul>
        <li>ZoeDepth and AdaBins showed minimal numerical deviation, but also low sensitivity to attacks ‚Äî revealing silent vulnerability.</li>
        <li>Monodepth2 and MiDaS exhibited high Ra and Ed, indicating stronger (and more visible) attack impact.</li>
      </ul>
      <p>üì∏ <strong>Figure</strong><br>images/models_compare.png ‚Äî Depth map disruptions caused by our patch and SPOO across all models.</p>
      <img src="images/models_compare.png" alt="Model Comparison Under Attack">

      <h4>b) Adversarial Training with DenseDepth</h4>
      <p>To enhance robustness, we adversarially trained DenseDepth using 9,450 patched images and corresponding ground truth depth maps. The goal was to restore accurate predictions despite adversarial interference.<br>
      Training lasted 50 epochs with a learning rate of 0.0001 and batch size of 16.<br>
      Metrics like Ed dropped from 0.0460 ‚Üí 0.0073 and SSIM rose to 0.9892.<br>
      Importantly, performance on clean images was not degraded.</p>
      <h3>Results Summary</h3>
      <ul>
        <li>DenseDepth RMSE: 0.0805 ‚Üí 0.0105</li>
        <li>SSIM: 0.8446 ‚Üí 0.9892</li>
        <li>Ed: 0.0460 ‚Üí 0.0073</li>
      </ul>
      <p>üì∏ <strong>Figure</strong><br>images/Patched (3).png ‚Äî Before and after adversarial training on the same patched image.</p>
      <img src="images/Patched (3).png" alt="Effect of Training on Our Patch">

      <h4>c) Testing Robustness on a Different Adversarial Patch</h4>
      <p>We tested the same adversarially trained DenseDepth model on a different patch (SPOO) to verify generalization. Results confirmed strong robustness:</p>
      <ul>
        <li>Ed improved from 0.0381 ‚Üí 0.0157</li>
        <li>Ra improved from 0.0639 ‚Üí 0.0120</li>
        <li>SSIM improved from 0.8896 ‚Üí 0.9608</li>
      </ul>
      <p>üì∏ <strong>Figure</strong><br>images/Patched (4).png ‚Äî Generalized defense against SPOO patch.</p>
      <img src="images/Patched (4).png" alt="Effect of Training on Existing Patch">


      <p><a href="Saahil_Project_Depth_Estimation_Against_Adversarial_Camouflage_in_AV.pdf" download>üìÑ Download Project PDF</a></p>
    </div>

<div class="project">
      <h2>üîê Intrusin Deetection System for Internet of Vehicles using ML, DL and Ensemble Methods</h2>

      <h3>Abstract</h3>
      <p>Modern advancements have laid the foundation for transforming vehicles into autonomous entities with sensors, network capabilities, and embedded software ‚Äî forming the Internet of Vehicles (IoV). IoV extends traditional VANET systems, enabling real-time interaction between vehicle components and their environments. However, this connectivity introduces significant cybersecurity vulnerabilities. Adversaries can exploit the system, causing data manipulation and critical safety threats. A robust Intrusion Detection System (IDS) is essential to mitigate these risks. This project compares traditional Machine Learning (ML), Deep Learning (DL), and Ensemble Learning (EL) models to determine the most effective techniques for IDS in AVs. We investigate three key questions: (1) Which traditional ML model performs best? (2) Can DL approaches outperform ML? (3) Is EL more effective than standalone models? We use the latest CICIoV2024 dataset to evaluate performance across Random Forest, Logistic Regression, AdaBoost, LSTM, and an ensemble model.</p>

      <h3>Methodology</h3>
      <p>The methodology follows a structured pipeline using the CICIoV2024 dataset, involving data preprocessing, model training, evaluation, and comparison. Three research questions are addressed:</p>
      <ul>
        <li><strong>RQ1:</strong> Traditional ML models (LR, AdaBoost, RF) are trained and evaluated to determine the most accurate for detecting and classifying IoV attacks.</li>
        <li><strong>RQ2:</strong> A Deep Learning model (LSTM) is trained and compared to ML models to assess performance improvements.</li>
        <li><strong>RQ3:</strong> An ensemble model (max voting of LR, RF, and AdaBoost) is evaluated to see if combining models improves IDS performance.</li>
      </ul>
      <img src="images/WPS Photos(1).png" alt="IoV IDS Methodology Diagram">

      <h3>Dataset Description</h3>
      <p><strong>Decimal Format:</strong> Includes message IDs and data bytes (DATA_0 to DATA_7). Classes include Benign, DoS, and Spoofing attacks (Gas, RPM, Speed, Steering Wheel). Each instance is labeled by category and specific class.<br>
      <strong>Binary Format:</strong> Binary representation of message IDs and data bits (ID0 to ID16 and DATA_00 to DATA_716). Allows bit-level analysis of traffic patterns.</p>
      <img src="images/dataset_description.png" alt="Dataset Table">

      <h3>Models Used</h3>
      <ul>
        <li><strong>Random Forest (RF):</strong> Builds multiple decision trees; ideal for high-dimensional data.</li>
        <li><strong>Logistic Regression (LR):</strong> Effective binary classifier; interpretable and fast.</li>
        <li><strong>AdaBoost:</strong> Boosts weak learners by focusing on misclassified instances for better accuracy.</li>
        <li><strong>Long Short-Term Memory (LSTM):</strong> Recurrent neural network well-suited for sequential CAN bus traffic.</li>
        <li><strong>Ensemble Learning:</strong> Uses max voting across LR, RF, and AdaBoost for improved robustness.</li>
      </ul>
      
      <h3>üß™ Experiments</h3>
      <p>The results presented below reflect the classification performance of ML, DL, and ensemble models using the CICIoV2024 dataset for binary, 3-class, and multi-class tasks. Metrics such as accuracy, recall, precision, and F1-score were analyzed, along with confusion matrices to provide detailed performance insight.</p>

      <h4>Binary Classification</h4>
      <img src="images/binary_results.png" alt="Binary Classification Results">
      <p>RF, AB, LSTM, and the ensemble achieved near-perfect performance with test accuracy of 0.999 and F1-scores of 1.00. LR performed slightly lower but still competitively. Confusion matrices confirmed that most models exhibited minimal misclassifications.</p>
      <img src="images/binary_confusion.png" alt="Binary Confusion Matrix">

      <h4>Three-Class Classification</h4>
      <img src="images/three_class_results.png" alt="Three-Class Classification Results">
      <p>RF, AB, and LSTM showed strong results again with 0.999 test accuracy and F1-scores of 1.00. LR lagged with lower recall and F1-score. Ensemble learning performed robustly across all metrics. The confusion matrix highlighted precise class separation for most models.</p>
      <img src="images/three_class_confusion.png" alt="Three-Class Confusion Matrix">

      <h4>Multi-Class Classification</h4>
      <img src="images/multi_class_results.png" alt="Multi-Class Classification Results">
      <p>RF and LSTM performed best in the multi-class setup with 0.996 test accuracy. LR and AB yielded slightly weaker results. Ensemble learning once again demonstrated high precision (1.00) and F1-score (0.95), confirming its strength across complex classification tasks.</p>
      <img src="images/multiclass_confusion.png" alt="Multiclass Confusion Matrix">

      <h4>Comparison with SOTA</h4>
      <img src="images/sota_comparison.png" alt="SOTA Comparison Table">
      <p>Our models outperformed several SOTA benchmarks. For the Decimal dataset, RF and LSTM matched or exceeded SOTA metrics, while ensemble models maintained the highest robustness. In the Binary dataset, LR and Ensemble approaches also surpassed SOTA models in both accuracy and F1-score. Use of cross-validation further ensured reliability and generalization of our findings.</p>

      <p><a href="Case_Study_Report-IDS-Project.docx" download>üìÑ Download Project DOCX</a></p>
    </div>

  </div>

  <footer>
    <p>¬© 2025 Saahil Khanna ‚Äî All rights reserved.</p>
  </footer>
</body>
</html>
